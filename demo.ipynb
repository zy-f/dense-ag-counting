{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/pc/Documents/stfu/classes/_ee292d/CrowdCounting-P2PNet')\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "from easydict import EasyDict\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from models.backbone import BackboneBase_VGG\n",
    "from models.matcher import build_matcher_crowd\n",
    "from models.p2pnet import P2PNet, SetCriterion_Crowd\n",
    "import models.vgg_ as vgg\n",
    "from crowd_datasets.SHHA.SHHA import random_crop\n",
    "from engine import train_one_epoch, evaluate_crowd_no_overlap, vis\n",
    "import util.misc as utils\n",
    "\n",
    "SEED = 10541 #0x292D\n",
    "VIS_DIR = 'vis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    # np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to load pretraining from elsewhere\n",
    "class CustomVGGBackboneWrapper(BackboneBase_VGG):\n",
    "    def __init__(self, name, return_interm_layers=True):\n",
    "        if name == 'vgg16_bn':\n",
    "            backbone = vgg.vgg16_bn()\n",
    "        elif name == 'vgg16':\n",
    "            backbone = vgg.vgg16()\n",
    "        num_channels = 256\n",
    "        super().__init__(backbone, num_channels, name, return_interm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_model(model_args, device, verbose=False,\n",
    "                   pretrain_layers=['backbone', 'fpn', 'classification', 'regression'],\n",
    "                   freeze_layers=[]):\n",
    "    num_classes = 1\n",
    "\n",
    "    backbone = CustomVGGBackboneWrapper(model_args.backbone)\n",
    "    model = P2PNet(backbone, model_args.row, model_args.line)\n",
    "    # load pretrained weights\n",
    "    if len(pretrain_layers) > 0:\n",
    "        checkpoint = torch.load('../CrowdCounting-P2PNet/weights/SHTechA.pth', map_location='cpu')\n",
    "        valid = lambda s: s[:s.find('.')] in pretrain_layers\n",
    "        valid_state = OrderedDict([(k,v) for k,v in checkpoint['model'].items() if valid(k)])\n",
    "        missing, unexpected = model.load_state_dict(valid_state, strict=False)\n",
    "        if verbose:\n",
    "            print(f\"pretrained weights loaded without the following keys:\\n{'\\n'.join(missing)}\")\n",
    "            if len(unexpected) > 0:\n",
    "                print(f\"WARNING: received unexpected keys:\\n{'\\n'.join(unexpected)}\")\n",
    "    for layer in freeze_layers:\n",
    "        for param in getattr(model, layer).parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    weight_dict = {'loss_ce': 1, 'loss_points': model_args.point_loss_coef}\n",
    "    losses = ['labels', 'points']\n",
    "    matcher = build_matcher_crowd(model_args)\n",
    "    criterion = SetCriterion_Crowd(num_classes, \\\n",
    "                                matcher=matcher, weight_dict=weight_dict, \\\n",
    "                                eos_coef=model_args.eos_coef, losses=losses)\n",
    "    return model.to(device), criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRANSFORM = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "class GrapeDataset(Dataset):\n",
    "    def __init__(self, root_path='labeled/', img_ext='.jpg', lbl_ext='_lbl.txt', \\\n",
    "                 transform=DEFAULT_TRANSFORM, train=False, patch=False, flip=False):\n",
    "        self.root = root_path + ('' if root_path[-1] == '/' else '/')\n",
    "        self.instances = [f[:-len(lbl_ext)] for f in os.listdir(self.root)\n",
    "                          if os.path.isfile(self.root+f) and f.endswith(lbl_ext)]\n",
    "        self.img_ext = img_ext\n",
    "        self.lbl_ext = lbl_ext\n",
    "        self.trf = transform\n",
    "        self.train = train\n",
    "        self.patch = patch\n",
    "        self.flip = flip\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "    \n",
    "    def _get_points(self, name):\n",
    "        label_path = self.root + name + self.lbl_ext\n",
    "        with open(label_path, 'r') as f:\n",
    "            coords_list = f.readlines()\n",
    "        coords = torch.tensor(tuple(tuple(map(float, line.split())) for line in coords_list))\n",
    "        return coords\n",
    "    \n",
    "    def _load_image(self, name):\n",
    "        img_path = self.root + name + self.img_ext\n",
    "        img = Image.open(img_path)\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        # only support single-instance indexing\n",
    "        img = self._load_image(self.instances[i]) # PIL image\n",
    "        point = self._get_points(self.instances[i]) # tensor\n",
    "        # perform val transforms on Image object\n",
    "        if not (self.train and self.patch): # if not cropping, handle resizing\n",
    "            # fix size of tensors (need to fix this)\n",
    "            # HACK: only batch one example at a time\n",
    "            width, height = img.size\n",
    "            new_width = width // 128 * 128\n",
    "            new_height = height // 128 * 128\n",
    "            img = img.resize((new_width, new_height), Image.LANCZOS)\n",
    "            point[:,0] *= new_width/width\n",
    "            point[:,1] *= new_height/height\n",
    "        # perform standard transforms\n",
    "        if self.trf is not None:\n",
    "            img = self.trf(img)\n",
    "        # additional transformation if training\n",
    "        if self.train:\n",
    "            # data augmentation -> random scale\n",
    "            scale_range = [0.7, 1.3]\n",
    "            min_size = min(img.shape[1:])\n",
    "            scale = random.uniform(*scale_range)\n",
    "            # scale the image and points\n",
    "            if scale * min_size > 128:\n",
    "                img = torch.nn.functional.upsample_bilinear(img.unsqueeze(0), scale_factor=scale).squeeze(0)\n",
    "                point *= scale\n",
    "            # random crop augumentation\n",
    "            if self.patch:\n",
    "                # generates a batch of images -> we just want one\n",
    "                imgs, points = random_crop(img, point, num_patch=1)\n",
    "                img, point = torch.tensor(imgs[0]), points[0]\n",
    "            # random flipping\n",
    "            if random.random() > 0.5 and self.flip:\n",
    "                # random flip\n",
    "                img = transforms.functional.hflip(img)\n",
    "                for i, _ in enumerate(point):\n",
    "                    point[:, 0] = 128 - point[:, 0]\n",
    "            \n",
    "        img = img.type(torch.float32)\n",
    "        # pack up related infos\n",
    "        img_id = torch.tensor(i, dtype=torch.int32, requires_grad=False)\n",
    "        target = {\n",
    "            'point': point,\n",
    "            'image_id': img_id,\n",
    "            'labels': torch.ones([point.shape[0]]).long()\n",
    "        }\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_kwargs = {\n",
    "    # 'batch_size': 128,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': True,\n",
    "    'collate_fn': utils.collate_fn\n",
    "}\n",
    "def get_subset_dataloaders(dset, subset_idx, k, bsz=128, dl_kwargs=dl_kwargs):\n",
    "    val_idx = subset_idx[k]\n",
    "    val_subset = Subset(dset, val_idx)\n",
    "    val_dl = DataLoader(val_subset, batch_size=1, **dl_kwargs) # HACK\n",
    "    train_idx = torch.cat((subset_idx[:k].flatten(), subset_idx[k+1:].flatten()))\n",
    "    trn_subset = Subset(dset, train_idx)\n",
    "    train_dl = DataLoader(trn_subset, shuffle=True, batch_size=bsz, **dl_kwargs)\n",
    "    trn_evaldl = DataLoader(trn_subset, batch_size=1, **dl_kwargs)\n",
    "    return train_dl, trn_evaldl, val_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = EasyDict(\n",
    "    lr_gen = 1e-4, # orig: 1e-4\n",
    "    lr_backbone = 1e-5, # orig: 1e-5\n",
    "    weight_decay = 1e-6, #orig: 1e-4\n",
    "    epochs = 200,\n",
    "    lr_drop = 50,\n",
    "    bsz = 64, # orig: 32\n",
    "    clip_max_norm = 0.5, #orig: 0.1\n",
    "    val_every = 10,\n",
    "    pretrain_layers = ['backbone', 'fpn'],\n",
    "    freeze_layers = ['backbone', 'fpn']\n",
    ")\n",
    "model_args = EasyDict(\n",
    "    # required to build inference model\n",
    "    backbone = 'vgg16_bn', # name of the convolutional backbone to use\n",
    "    pretrained = False,\n",
    "    row = 1, # row number of anchor points\n",
    "    line = 1, # line number of anchor points\n",
    "    # required for model training\n",
    "    point_loss_coef = 2e-4, # orig: 2e-4\n",
    "    eos_coef = 0.3, #orig: 0.5 # Relative classification weight of the no-object class\n",
    "    set_cost_class = 1, # for matcher -- Class coefficient in the matching cost\n",
    "    set_cost_point = .05 #orig: .05 # for matcher -- L1 point coefficient in the matching cost\n",
    ")\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "base_dset = GrapeDataset(train=False, patch=False, flip=False)\n",
    "K = 7\n",
    "assert len(base_dset) % K == 0 # kfold splits must be exactly even"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> FOLD 1/7 <<<\n",
      "EPOCH 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pc/mambaforge/envs/ag_ct/lib/python3.12/site-packages/torch/nn/functional.py:4185: UserWarning: nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/var/folders/cp/ps4v9yh52dj0wy2dnwf912cr0000gn/T/ipykernel_12788/4278468065.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_id = torch.tensor(i, dtype=torch.int32, requires_grad=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averaged stats: lr: 0.000100  loss: 0.6401 (0.6602)  loss_ce: 0.6401 (0.6602)  loss_ce_unscaled: 0.6401 (0.6602)  loss_point_unscaled: 14.2951 (14.5226)\n",
      "EPOCH 2/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4726 (0.5275)  loss_ce: 0.4726 (0.5275)  loss_ce_unscaled: 0.4726 (0.5275)  loss_point_unscaled: 14.4699 (15.0496)\n",
      "EPOCH 3/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4270 (0.4642)  loss_ce: 0.4270 (0.4642)  loss_ce_unscaled: 0.4270 (0.4642)  loss_point_unscaled: 15.5190 (17.1959)\n",
      "EPOCH 4/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4879 (0.5257)  loss_ce: 0.4879 (0.5257)  loss_ce_unscaled: 0.4879 (0.5257)  loss_point_unscaled: 16.4108 (16.4371)\n",
      "EPOCH 5/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.5100 (0.5475)  loss_ce: 0.5100 (0.5475)  loss_ce_unscaled: 0.5100 (0.5475)  loss_point_unscaled: 16.9774 (17.0763)\n",
      "EPOCH 6/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4655 (0.4704)  loss_ce: 0.4655 (0.4704)  loss_ce_unscaled: 0.4655 (0.4704)  loss_point_unscaled: 16.4168 (16.9210)\n",
      "EPOCH 7/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4645 (0.4730)  loss_ce: 0.4645 (0.4730)  loss_ce_unscaled: 0.4645 (0.4730)  loss_point_unscaled: 13.8274 (14.5352)\n",
      "EPOCH 8/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4516 (0.4695)  loss_ce: 0.4516 (0.4695)  loss_ce_unscaled: 0.4516 (0.4695)  loss_point_unscaled: 15.0327 (15.2562)\n",
      "EPOCH 9/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4112 (0.4414)  loss_ce: 0.4112 (0.4414)  loss_ce_unscaled: 0.4112 (0.4414)  loss_point_unscaled: 14.5188 (15.2137)\n",
      "EPOCH 10/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4415 (0.4507)  loss_ce: 0.4415 (0.4507)  loss_ce_unscaled: 0.4415 (0.4507)  loss_point_unscaled: 14.3183 (14.7990)\n",
      "trn: mae=46.205128205128204, mse=53.74679477622781\n",
      "val: mae=38.30769230769231, mse=43.55368040617322\n",
      "EPOCH 11/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4242 (0.4703)  loss_ce: 0.4242 (0.4703)  loss_ce_unscaled: 0.4242 (0.4703)  loss_point_unscaled: 15.5318 (15.9962)\n",
      "EPOCH 12/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4113 (0.4464)  loss_ce: 0.4113 (0.4464)  loss_ce_unscaled: 0.4113 (0.4464)  loss_point_unscaled: 15.7438 (16.2854)\n",
      "EPOCH 13/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.4023 (0.4211)  loss_ce: 0.4023 (0.4211)  loss_ce_unscaled: 0.4023 (0.4211)  loss_point_unscaled: 17.0651 (17.6108)\n",
      "EPOCH 14/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3930 (0.4206)  loss_ce: 0.3930 (0.4206)  loss_ce_unscaled: 0.3930 (0.4206)  loss_point_unscaled: 17.5535 (17.8131)\n",
      "EPOCH 15/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3118 (0.3610)  loss_ce: 0.3118 (0.3610)  loss_ce_unscaled: 0.3118 (0.3610)  loss_point_unscaled: 18.3483 (19.0255)\n",
      "EPOCH 16/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3645 (0.3773)  loss_ce: 0.3645 (0.3773)  loss_ce_unscaled: 0.3645 (0.3773)  loss_point_unscaled: 18.2837 (19.7314)\n",
      "EPOCH 17/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3689 (0.3831)  loss_ce: 0.3689 (0.3831)  loss_ce_unscaled: 0.3689 (0.3831)  loss_point_unscaled: 20.5898 (22.0531)\n",
      "EPOCH 18/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3183 (0.3523)  loss_ce: 0.3183 (0.3523)  loss_ce_unscaled: 0.3183 (0.3523)  loss_point_unscaled: 24.2091 (25.0923)\n",
      "EPOCH 19/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3393 (0.3478)  loss_ce: 0.3393 (0.3478)  loss_ce_unscaled: 0.3393 (0.3478)  loss_point_unscaled: 28.1784 (30.4194)\n",
      "EPOCH 20/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3294 (0.3368)  loss_ce: 0.3294 (0.3368)  loss_ce_unscaled: 0.3294 (0.3368)  loss_point_unscaled: 27.0755 (28.0726)\n",
      "trn: mae=12.897435897435898, mse=17.301993363742064\n",
      "val: mae=15.384615384615385, mse=21.950994370327596\n",
      "EPOCH 21/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.3111 (0.3128)  loss_ce: 0.3111 (0.3128)  loss_ce_unscaled: 0.3111 (0.3128)  loss_point_unscaled: 36.9926 (37.2077)\n",
      "EPOCH 22/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2829 (0.2896)  loss_ce: 0.2829 (0.2896)  loss_ce_unscaled: 0.2829 (0.2896)  loss_point_unscaled: 36.5599 (36.9891)\n",
      "EPOCH 23/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2363 (0.2671)  loss_ce: 0.2363 (0.2671)  loss_ce_unscaled: 0.2363 (0.2671)  loss_point_unscaled: 40.2790 (41.9138)\n",
      "EPOCH 24/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2092 (0.2494)  loss_ce: 0.2092 (0.2494)  loss_ce_unscaled: 0.2092 (0.2494)  loss_point_unscaled: 35.5039 (43.1755)\n",
      "EPOCH 25/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2007 (0.2392)  loss_ce: 0.2007 (0.2392)  loss_ce_unscaled: 0.2007 (0.2392)  loss_point_unscaled: 35.5337 (41.5089)\n",
      "EPOCH 26/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2384 (0.2472)  loss_ce: 0.2384 (0.2472)  loss_ce_unscaled: 0.2384 (0.2472)  loss_point_unscaled: 43.9487 (46.0761)\n",
      "EPOCH 27/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2404 (0.2813)  loss_ce: 0.2404 (0.2813)  loss_ce_unscaled: 0.2404 (0.2813)  loss_point_unscaled: 44.7745 (46.9611)\n",
      "EPOCH 28/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2420 (0.2544)  loss_ce: 0.2420 (0.2544)  loss_ce_unscaled: 0.2420 (0.2544)  loss_point_unscaled: 44.3064 (46.9731)\n",
      "EPOCH 29/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2263 (0.2388)  loss_ce: 0.2263 (0.2388)  loss_ce_unscaled: 0.2263 (0.2388)  loss_point_unscaled: 41.3099 (44.2735)\n",
      "EPOCH 30/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2276 (0.2362)  loss_ce: 0.2276 (0.2362)  loss_ce_unscaled: 0.2276 (0.2362)  loss_point_unscaled: 42.0485 (43.4216)\n",
      "trn: mae=9.743589743589743, mse=12.880117852622153\n",
      "val: mae=16.615384615384617, mse=21.80331661432763\n",
      "EPOCH 31/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2237 (0.2288)  loss_ce: 0.2237 (0.2288)  loss_ce_unscaled: 0.2237 (0.2288)  loss_point_unscaled: 40.3089 (42.7684)\n",
      "EPOCH 32/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2451 (0.2605)  loss_ce: 0.2451 (0.2605)  loss_ce_unscaled: 0.2451 (0.2605)  loss_point_unscaled: 35.5809 (39.9949)\n",
      "EPOCH 33/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2422 (0.2666)  loss_ce: 0.2422 (0.2666)  loss_ce_unscaled: 0.2422 (0.2666)  loss_point_unscaled: 39.2588 (42.5424)\n",
      "EPOCH 34/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2301 (0.2323)  loss_ce: 0.2301 (0.2323)  loss_ce_unscaled: 0.2301 (0.2323)  loss_point_unscaled: 43.0522 (47.1752)\n",
      "EPOCH 35/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2210 (0.2216)  loss_ce: 0.2210 (0.2216)  loss_ce_unscaled: 0.2210 (0.2216)  loss_point_unscaled: 40.9426 (42.4154)\n",
      "EPOCH 36/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1970 (0.2113)  loss_ce: 0.1970 (0.2113)  loss_ce_unscaled: 0.1970 (0.2113)  loss_point_unscaled: 40.6300 (45.3205)\n",
      "EPOCH 37/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2274 (0.2286)  loss_ce: 0.2274 (0.2286)  loss_ce_unscaled: 0.2274 (0.2286)  loss_point_unscaled: 39.5903 (43.5780)\n",
      "EPOCH 38/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2357 (0.2446)  loss_ce: 0.2357 (0.2446)  loss_ce_unscaled: 0.2357 (0.2446)  loss_point_unscaled: 41.1886 (42.3856)\n",
      "EPOCH 39/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2333 (0.2430)  loss_ce: 0.2333 (0.2430)  loss_ce_unscaled: 0.2333 (0.2430)  loss_point_unscaled: 42.0169 (43.7725)\n",
      "EPOCH 40/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2067 (0.2221)  loss_ce: 0.2067 (0.2221)  loss_ce_unscaled: 0.2067 (0.2221)  loss_point_unscaled: 41.4070 (42.2690)\n",
      "trn: mae=8.974358974358974, mse=12.21495510450727\n",
      "val: mae=14.384615384615385, mse=19.460018183247097\n",
      "EPOCH 41/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1787 (0.2010)  loss_ce: 0.1787 (0.2010)  loss_ce_unscaled: 0.1787 (0.2010)  loss_point_unscaled: 43.3231 (45.2104)\n",
      "EPOCH 42/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2300 (0.2620)  loss_ce: 0.2300 (0.2620)  loss_ce_unscaled: 0.2300 (0.2620)  loss_point_unscaled: 41.1238 (46.1326)\n",
      "EPOCH 43/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2143 (0.2241)  loss_ce: 0.2143 (0.2241)  loss_ce_unscaled: 0.2143 (0.2241)  loss_point_unscaled: 44.3480 (45.8560)\n",
      "EPOCH 44/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2158 (0.2165)  loss_ce: 0.2158 (0.2165)  loss_ce_unscaled: 0.2158 (0.2165)  loss_point_unscaled: 43.8607 (45.2807)\n",
      "EPOCH 45/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2122 (0.2347)  loss_ce: 0.2122 (0.2347)  loss_ce_unscaled: 0.2122 (0.2347)  loss_point_unscaled: 37.8954 (39.4425)\n",
      "EPOCH 46/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2034 (0.2273)  loss_ce: 0.2034 (0.2273)  loss_ce_unscaled: 0.2034 (0.2273)  loss_point_unscaled: 41.8293 (42.1347)\n",
      "EPOCH 47/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2100 (0.2178)  loss_ce: 0.2100 (0.2178)  loss_ce_unscaled: 0.2100 (0.2178)  loss_point_unscaled: 43.1144 (45.2323)\n",
      "EPOCH 48/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2001 (0.2046)  loss_ce: 0.2001 (0.2046)  loss_ce_unscaled: 0.2001 (0.2046)  loss_point_unscaled: 41.7131 (48.1865)\n",
      "EPOCH 49/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1991)  loss_ce: 0.1884 (0.1991)  loss_ce_unscaled: 0.1884 (0.1991)  loss_point_unscaled: 40.2114 (42.1257)\n",
      "EPOCH 50/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2066 (0.2231)  loss_ce: 0.2066 (0.2231)  loss_ce_unscaled: 0.2066 (0.2231)  loss_point_unscaled: 40.8775 (45.4469)\n",
      "trn: mae=9.85897435897436, mse=13.891982561433752\n",
      "val: mae=12.23076923076923, mse=19.125094268244293\n",
      "EPOCH 51/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2209 (0.2279)  loss_ce: 0.2209 (0.2279)  loss_ce_unscaled: 0.2209 (0.2279)  loss_point_unscaled: 41.3644 (43.2476)\n",
      "EPOCH 52/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2046 (0.2357)  loss_ce: 0.2046 (0.2357)  loss_ce_unscaled: 0.2046 (0.2357)  loss_point_unscaled: 46.5319 (47.9377)\n",
      "EPOCH 53/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2169 (0.2237)  loss_ce: 0.2169 (0.2237)  loss_ce_unscaled: 0.2169 (0.2237)  loss_point_unscaled: 43.9165 (51.5574)\n",
      "EPOCH 54/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1995 (0.2048)  loss_ce: 0.1995 (0.2048)  loss_ce_unscaled: 0.1995 (0.2048)  loss_point_unscaled: 42.6208 (43.9348)\n",
      "EPOCH 55/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1759 (0.1928)  loss_ce: 0.1759 (0.1928)  loss_ce_unscaled: 0.1759 (0.1928)  loss_point_unscaled: 41.4257 (44.0735)\n",
      "EPOCH 56/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1876 (0.1940)  loss_ce: 0.1876 (0.1940)  loss_ce_unscaled: 0.1876 (0.1940)  loss_point_unscaled: 36.8692 (38.9146)\n",
      "EPOCH 57/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1606 (0.1916)  loss_ce: 0.1606 (0.1916)  loss_ce_unscaled: 0.1606 (0.1916)  loss_point_unscaled: 35.5953 (39.9527)\n",
      "EPOCH 58/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1958 (0.2123)  loss_ce: 0.1958 (0.2123)  loss_ce_unscaled: 0.1958 (0.2123)  loss_point_unscaled: 48.7684 (49.7052)\n",
      "EPOCH 59/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1627 (0.1701)  loss_ce: 0.1627 (0.1701)  loss_ce_unscaled: 0.1627 (0.1701)  loss_point_unscaled: 40.0638 (43.8979)\n",
      "EPOCH 60/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2023 (0.2057)  loss_ce: 0.2023 (0.2057)  loss_ce_unscaled: 0.2023 (0.2057)  loss_point_unscaled: 40.3236 (42.4478)\n",
      "trn: mae=10.320512820512821, mse=14.220335745612644\n",
      "val: mae=11.76923076923077, mse=18.100148745670147\n",
      "EPOCH 61/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1988 (0.2096)  loss_ce: 0.1988 (0.2096)  loss_ce_unscaled: 0.1988 (0.2096)  loss_point_unscaled: 39.4667 (42.6302)\n",
      "EPOCH 62/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1930 (0.1985)  loss_ce: 0.1930 (0.1985)  loss_ce_unscaled: 0.1930 (0.1985)  loss_point_unscaled: 38.2527 (42.8303)\n",
      "EPOCH 63/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1571 (0.1788)  loss_ce: 0.1571 (0.1788)  loss_ce_unscaled: 0.1571 (0.1788)  loss_point_unscaled: 34.0389 (39.5692)\n",
      "EPOCH 64/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1835 (0.1868)  loss_ce: 0.1835 (0.1868)  loss_ce_unscaled: 0.1835 (0.1868)  loss_point_unscaled: 42.8353 (44.7651)\n",
      "EPOCH 65/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1902 (0.1950)  loss_ce: 0.1902 (0.1950)  loss_ce_unscaled: 0.1902 (0.1950)  loss_point_unscaled: 43.9643 (50.0404)\n",
      "EPOCH 66/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1908 (0.2114)  loss_ce: 0.1908 (0.2114)  loss_ce_unscaled: 0.1908 (0.2114)  loss_point_unscaled: 43.4086 (44.7906)\n",
      "EPOCH 67/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1380 (0.1663)  loss_ce: 0.1380 (0.1663)  loss_ce_unscaled: 0.1380 (0.1663)  loss_point_unscaled: 38.3824 (40.9470)\n",
      "EPOCH 68/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1927 (0.1962)  loss_ce: 0.1927 (0.1962)  loss_ce_unscaled: 0.1927 (0.1962)  loss_point_unscaled: 44.4007 (47.1265)\n",
      "EPOCH 69/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1737 (0.1789)  loss_ce: 0.1737 (0.1789)  loss_ce_unscaled: 0.1737 (0.1789)  loss_point_unscaled: 43.6170 (44.7274)\n",
      "EPOCH 70/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1766 (0.1792)  loss_ce: 0.1766 (0.1792)  loss_ce_unscaled: 0.1766 (0.1792)  loss_point_unscaled: 41.1813 (43.7091)\n",
      "trn: mae=10.179487179487179, mse=13.621250366191113\n",
      "val: mae=11.307692307692308, mse=15.721127381770245\n",
      "EPOCH 71/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1702 (0.1932)  loss_ce: 0.1702 (0.1932)  loss_ce_unscaled: 0.1702 (0.1932)  loss_point_unscaled: 46.0298 (50.6652)\n",
      "EPOCH 72/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1704 (0.1793)  loss_ce: 0.1704 (0.1793)  loss_ce_unscaled: 0.1704 (0.1793)  loss_point_unscaled: 44.7823 (45.0136)\n",
      "EPOCH 73/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2039 (0.2415)  loss_ce: 0.2039 (0.2415)  loss_ce_unscaled: 0.2039 (0.2415)  loss_point_unscaled: 43.9033 (44.1361)\n",
      "EPOCH 74/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1536 (0.1766)  loss_ce: 0.1536 (0.1766)  loss_ce_unscaled: 0.1536 (0.1766)  loss_point_unscaled: 45.9350 (48.9983)\n",
      "EPOCH 75/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1672 (0.1926)  loss_ce: 0.1672 (0.1926)  loss_ce_unscaled: 0.1672 (0.1926)  loss_point_unscaled: 42.4639 (45.2258)\n",
      "EPOCH 76/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1861)  loss_ce: 0.1735 (0.1861)  loss_ce_unscaled: 0.1735 (0.1861)  loss_point_unscaled: 36.3296 (43.4584)\n",
      "EPOCH 77/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1893 (0.1908)  loss_ce: 0.1893 (0.1908)  loss_ce_unscaled: 0.1893 (0.1908)  loss_point_unscaled: 45.4002 (47.7185)\n",
      "EPOCH 78/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1976 (0.2082)  loss_ce: 0.1976 (0.2082)  loss_ce_unscaled: 0.1976 (0.2082)  loss_point_unscaled: 43.4256 (43.8473)\n",
      "EPOCH 79/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1709 (0.1776)  loss_ce: 0.1709 (0.1776)  loss_ce_unscaled: 0.1709 (0.1776)  loss_point_unscaled: 38.1092 (40.7792)\n",
      "EPOCH 80/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1866 (0.1875)  loss_ce: 0.1866 (0.1875)  loss_ce_unscaled: 0.1866 (0.1875)  loss_point_unscaled: 42.2396 (42.7757)\n",
      "trn: mae=9.628205128205128, mse=13.159280628767922\n",
      "val: mae=11.23076923076923, mse=17.780715569229663\n",
      "EPOCH 81/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1878 (0.1913)  loss_ce: 0.1878 (0.1913)  loss_ce_unscaled: 0.1878 (0.1913)  loss_point_unscaled: 43.9868 (47.2038)\n",
      "EPOCH 82/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1582 (0.1735)  loss_ce: 0.1582 (0.1735)  loss_ce_unscaled: 0.1582 (0.1735)  loss_point_unscaled: 44.1636 (46.5405)\n",
      "EPOCH 83/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1570 (0.1721)  loss_ce: 0.1570 (0.1721)  loss_ce_unscaled: 0.1570 (0.1721)  loss_point_unscaled: 42.6479 (42.6688)\n",
      "EPOCH 84/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1610 (0.1682)  loss_ce: 0.1610 (0.1682)  loss_ce_unscaled: 0.1610 (0.1682)  loss_point_unscaled: 40.5047 (42.9553)\n",
      "EPOCH 85/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1772 (0.1836)  loss_ce: 0.1772 (0.1836)  loss_ce_unscaled: 0.1772 (0.1836)  loss_point_unscaled: 39.3765 (41.8065)\n",
      "EPOCH 86/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1735 (0.1955)  loss_ce: 0.1735 (0.1955)  loss_ce_unscaled: 0.1735 (0.1955)  loss_point_unscaled: 42.3696 (43.0687)\n",
      "EPOCH 87/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1660 (0.1803)  loss_ce: 0.1660 (0.1803)  loss_ce_unscaled: 0.1660 (0.1803)  loss_point_unscaled: 45.2158 (47.3351)\n",
      "EPOCH 88/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1679 (0.1720)  loss_ce: 0.1679 (0.1720)  loss_ce_unscaled: 0.1679 (0.1720)  loss_point_unscaled: 42.0955 (44.7856)\n",
      "EPOCH 89/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1673 (0.1710)  loss_ce: 0.1673 (0.1710)  loss_ce_unscaled: 0.1673 (0.1710)  loss_point_unscaled: 38.8844 (40.7044)\n",
      "EPOCH 90/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1756 (0.1816)  loss_ce: 0.1756 (0.1816)  loss_ce_unscaled: 0.1756 (0.1816)  loss_point_unscaled: 40.2625 (40.9074)\n",
      "trn: mae=8.512820512820513, mse=12.17395285867036\n",
      "val: mae=11.076923076923077, mse=18.085268122892817\n",
      "EPOCH 91/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1839 (0.1934)  loss_ce: 0.1839 (0.1934)  loss_ce_unscaled: 0.1839 (0.1934)  loss_point_unscaled: 44.0417 (44.0721)\n",
      "EPOCH 92/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1745 (0.1861)  loss_ce: 0.1745 (0.1861)  loss_ce_unscaled: 0.1745 (0.1861)  loss_point_unscaled: 44.9716 (46.9018)\n",
      "EPOCH 93/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1786 (0.1860)  loss_ce: 0.1786 (0.1860)  loss_ce_unscaled: 0.1786 (0.1860)  loss_point_unscaled: 41.4723 (45.1761)\n",
      "EPOCH 94/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1415 (0.1684)  loss_ce: 0.1415 (0.1684)  loss_ce_unscaled: 0.1415 (0.1684)  loss_point_unscaled: 45.7476 (45.9760)\n",
      "EPOCH 95/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1572 (0.1671)  loss_ce: 0.1572 (0.1671)  loss_ce_unscaled: 0.1572 (0.1671)  loss_point_unscaled: 43.7008 (46.8643)\n",
      "EPOCH 96/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1897 (0.1988)  loss_ce: 0.1897 (0.1988)  loss_ce_unscaled: 0.1897 (0.1988)  loss_point_unscaled: 43.1001 (43.8199)\n",
      "EPOCH 97/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1378 (0.1532)  loss_ce: 0.1378 (0.1532)  loss_ce_unscaled: 0.1378 (0.1532)  loss_point_unscaled: 39.3458 (42.0064)\n",
      "EPOCH 98/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1681 (0.1883)  loss_ce: 0.1681 (0.1883)  loss_ce_unscaled: 0.1681 (0.1883)  loss_point_unscaled: 43.3795 (47.2893)\n",
      "EPOCH 99/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1872 (0.1960)  loss_ce: 0.1872 (0.1960)  loss_ce_unscaled: 0.1872 (0.1960)  loss_point_unscaled: 40.5484 (44.1194)\n",
      "EPOCH 100/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1672 (0.1684)  loss_ce: 0.1672 (0.1684)  loss_ce_unscaled: 0.1672 (0.1684)  loss_point_unscaled: 41.5038 (41.8540)\n",
      "trn: mae=9.987179487179487, mse=13.3805254582345\n",
      "val: mae=10.76923076923077, mse=14.992305718908975\n",
      "EPOCH 101/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1704 (0.1830)  loss_ce: 0.1704 (0.1830)  loss_ce_unscaled: 0.1704 (0.1830)  loss_point_unscaled: 46.5234 (46.5999)\n",
      "EPOCH 102/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1515 (0.1630)  loss_ce: 0.1515 (0.1630)  loss_ce_unscaled: 0.1515 (0.1630)  loss_point_unscaled: 44.6895 (48.6550)\n",
      "EPOCH 103/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1622 (0.1671)  loss_ce: 0.1622 (0.1671)  loss_ce_unscaled: 0.1622 (0.1671)  loss_point_unscaled: 40.7137 (41.0551)\n",
      "EPOCH 104/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1691 (0.2015)  loss_ce: 0.1691 (0.2015)  loss_ce_unscaled: 0.1691 (0.2015)  loss_point_unscaled: 44.6935 (46.4638)\n",
      "EPOCH 105/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1549 (0.1634)  loss_ce: 0.1549 (0.1634)  loss_ce_unscaled: 0.1549 (0.1634)  loss_point_unscaled: 43.6918 (46.5371)\n",
      "EPOCH 106/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1605 (0.1750)  loss_ce: 0.1605 (0.1750)  loss_ce_unscaled: 0.1605 (0.1750)  loss_point_unscaled: 42.6316 (45.5278)\n",
      "EPOCH 107/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1373 (0.1592)  loss_ce: 0.1373 (0.1592)  loss_ce_unscaled: 0.1373 (0.1592)  loss_point_unscaled: 33.8376 (39.2965)\n",
      "EPOCH 108/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1646 (0.1741)  loss_ce: 0.1646 (0.1741)  loss_ce_unscaled: 0.1646 (0.1741)  loss_point_unscaled: 35.4151 (41.4771)\n",
      "EPOCH 109/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1418 (0.1510)  loss_ce: 0.1418 (0.1510)  loss_ce_unscaled: 0.1418 (0.1510)  loss_point_unscaled: 34.0201 (38.6673)\n",
      "EPOCH 110/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1611 (0.1838)  loss_ce: 0.1611 (0.1838)  loss_ce_unscaled: 0.1611 (0.1838)  loss_point_unscaled: 44.5570 (44.6814)\n",
      "trn: mae=9.371794871794872, mse=13.460769208789952\n",
      "val: mae=12.0, mse=19.11101172703238\n",
      "EPOCH 111/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1601 (0.1620)  loss_ce: 0.1601 (0.1620)  loss_ce_unscaled: 0.1601 (0.1620)  loss_point_unscaled: 46.2481 (47.5445)\n",
      "EPOCH 112/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1507 (0.1585)  loss_ce: 0.1507 (0.1585)  loss_ce_unscaled: 0.1507 (0.1585)  loss_point_unscaled: 36.5799 (40.8820)\n",
      "EPOCH 113/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1797 (0.1798)  loss_ce: 0.1797 (0.1798)  loss_ce_unscaled: 0.1797 (0.1798)  loss_point_unscaled: 46.1042 (54.4030)\n",
      "EPOCH 114/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1362 (0.1502)  loss_ce: 0.1362 (0.1502)  loss_ce_unscaled: 0.1362 (0.1502)  loss_point_unscaled: 35.1406 (39.0345)\n",
      "EPOCH 115/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1584 (0.1676)  loss_ce: 0.1584 (0.1676)  loss_ce_unscaled: 0.1584 (0.1676)  loss_point_unscaled: 46.5190 (49.1800)\n",
      "EPOCH 116/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1214 (0.1498)  loss_ce: 0.1214 (0.1498)  loss_ce_unscaled: 0.1214 (0.1498)  loss_point_unscaled: 44.7837 (46.4666)\n",
      "EPOCH 117/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1507 (0.1598)  loss_ce: 0.1507 (0.1598)  loss_ce_unscaled: 0.1507 (0.1598)  loss_point_unscaled: 44.1713 (45.4145)\n",
      "EPOCH 118/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1884 (0.1913)  loss_ce: 0.1884 (0.1913)  loss_ce_unscaled: 0.1884 (0.1913)  loss_point_unscaled: 42.4397 (44.9703)\n",
      "EPOCH 119/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1709 (0.1766)  loss_ce: 0.1709 (0.1766)  loss_ce_unscaled: 0.1709 (0.1766)  loss_point_unscaled: 37.3939 (42.0490)\n",
      "EPOCH 120/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1543 (0.1640)  loss_ce: 0.1543 (0.1640)  loss_ce_unscaled: 0.1543 (0.1640)  loss_point_unscaled: 46.6122 (47.7397)\n",
      "trn: mae=10.076923076923077, mse=14.099463528619513\n",
      "val: mae=11.923076923076923, mse=18.214533504024494\n",
      "EPOCH 121/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1633 (0.1668)  loss_ce: 0.1633 (0.1668)  loss_ce_unscaled: 0.1633 (0.1668)  loss_point_unscaled: 37.1004 (38.5346)\n",
      "EPOCH 122/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1700 (0.1765)  loss_ce: 0.1700 (0.1765)  loss_ce_unscaled: 0.1700 (0.1765)  loss_point_unscaled: 34.3632 (37.3576)\n",
      "EPOCH 123/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1587 (0.1677)  loss_ce: 0.1587 (0.1677)  loss_ce_unscaled: 0.1587 (0.1677)  loss_point_unscaled: 34.9325 (37.8683)\n",
      "EPOCH 124/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1711 (0.1720)  loss_ce: 0.1711 (0.1720)  loss_ce_unscaled: 0.1711 (0.1720)  loss_point_unscaled: 41.5891 (45.6684)\n",
      "EPOCH 125/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1552 (0.1561)  loss_ce: 0.1552 (0.1561)  loss_ce_unscaled: 0.1552 (0.1561)  loss_point_unscaled: 39.0611 (40.7131)\n",
      "EPOCH 126/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1348 (0.1501)  loss_ce: 0.1348 (0.1501)  loss_ce_unscaled: 0.1348 (0.1501)  loss_point_unscaled: 41.8687 (46.9438)\n",
      "EPOCH 127/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1681 (0.1771)  loss_ce: 0.1681 (0.1771)  loss_ce_unscaled: 0.1681 (0.1771)  loss_point_unscaled: 46.8002 (54.3301)\n",
      "EPOCH 128/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1436 (0.1500)  loss_ce: 0.1436 (0.1500)  loss_ce_unscaled: 0.1436 (0.1500)  loss_point_unscaled: 40.5714 (41.4329)\n",
      "EPOCH 129/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.2008 (0.2085)  loss_ce: 0.2008 (0.2085)  loss_ce_unscaled: 0.2008 (0.2085)  loss_point_unscaled: 42.2292 (42.9316)\n",
      "EPOCH 130/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1530 (0.1534)  loss_ce: 0.1530 (0.1534)  loss_ce_unscaled: 0.1530 (0.1534)  loss_point_unscaled: 41.2788 (42.5188)\n",
      "trn: mae=7.282051282051282, mse=10.441534419118726\n",
      "val: mae=11.0, mse=15.978350738035898\n",
      "EPOCH 131/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1649 (0.1895)  loss_ce: 0.1649 (0.1895)  loss_ce_unscaled: 0.1649 (0.1895)  loss_point_unscaled: 42.7302 (44.2518)\n",
      "EPOCH 132/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1605 (0.1664)  loss_ce: 0.1605 (0.1664)  loss_ce_unscaled: 0.1605 (0.1664)  loss_point_unscaled: 31.9435 (35.5313)\n",
      "EPOCH 133/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1529 (0.1597)  loss_ce: 0.1529 (0.1597)  loss_ce_unscaled: 0.1529 (0.1597)  loss_point_unscaled: 41.2386 (43.9111)\n",
      "EPOCH 134/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1464 (0.1623)  loss_ce: 0.1464 (0.1623)  loss_ce_unscaled: 0.1464 (0.1623)  loss_point_unscaled: 38.5627 (40.0788)\n",
      "EPOCH 135/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1565 (0.1621)  loss_ce: 0.1565 (0.1621)  loss_ce_unscaled: 0.1565 (0.1621)  loss_point_unscaled: 40.5805 (42.7752)\n",
      "EPOCH 136/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1279 (0.1556)  loss_ce: 0.1279 (0.1556)  loss_ce_unscaled: 0.1279 (0.1556)  loss_point_unscaled: 47.5453 (48.6597)\n",
      "EPOCH 137/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1482 (0.1699)  loss_ce: 0.1482 (0.1699)  loss_ce_unscaled: 0.1482 (0.1699)  loss_point_unscaled: 39.9271 (41.3091)\n",
      "EPOCH 138/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1468 (0.1488)  loss_ce: 0.1468 (0.1488)  loss_ce_unscaled: 0.1468 (0.1488)  loss_point_unscaled: 41.1534 (43.4297)\n",
      "EPOCH 139/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1253 (0.1469)  loss_ce: 0.1253 (0.1469)  loss_ce_unscaled: 0.1253 (0.1469)  loss_point_unscaled: 36.7410 (41.1860)\n",
      "EPOCH 140/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1522 (0.1781)  loss_ce: 0.1522 (0.1781)  loss_ce_unscaled: 0.1522 (0.1781)  loss_point_unscaled: 44.7888 (45.5472)\n",
      "trn: mae=10.012820512820513, mse=13.511153986661615\n",
      "val: mae=11.0, mse=17.950787426482695\n",
      "EPOCH 141/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1345 (0.1401)  loss_ce: 0.1345 (0.1401)  loss_ce_unscaled: 0.1345 (0.1401)  loss_point_unscaled: 45.2685 (50.5563)\n",
      "EPOCH 142/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1488 (0.1517)  loss_ce: 0.1488 (0.1517)  loss_ce_unscaled: 0.1488 (0.1517)  loss_point_unscaled: 33.2784 (40.4619)\n",
      "EPOCH 143/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1507 (0.1598)  loss_ce: 0.1507 (0.1598)  loss_ce_unscaled: 0.1507 (0.1598)  loss_point_unscaled: 44.9897 (50.5428)\n",
      "EPOCH 144/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1361 (0.1532)  loss_ce: 0.1361 (0.1532)  loss_ce_unscaled: 0.1361 (0.1532)  loss_point_unscaled: 34.6288 (40.0057)\n",
      "EPOCH 145/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1507 (0.1931)  loss_ce: 0.1507 (0.1931)  loss_ce_unscaled: 0.1507 (0.1931)  loss_point_unscaled: 40.5044 (41.4154)\n",
      "EPOCH 146/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1438 (0.1478)  loss_ce: 0.1438 (0.1478)  loss_ce_unscaled: 0.1438 (0.1478)  loss_point_unscaled: 40.2643 (44.6899)\n",
      "EPOCH 147/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1629 (0.1635)  loss_ce: 0.1629 (0.1635)  loss_ce_unscaled: 0.1629 (0.1635)  loss_point_unscaled: 45.1058 (45.7050)\n",
      "EPOCH 148/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1554 (0.1560)  loss_ce: 0.1554 (0.1560)  loss_ce_unscaled: 0.1554 (0.1560)  loss_point_unscaled: 39.4272 (43.7830)\n",
      "EPOCH 149/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1431 (0.1575)  loss_ce: 0.1431 (0.1575)  loss_ce_unscaled: 0.1431 (0.1575)  loss_point_unscaled: 42.4070 (46.2833)\n",
      "EPOCH 150/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1662 (0.1672)  loss_ce: 0.1662 (0.1672)  loss_ce_unscaled: 0.1662 (0.1672)  loss_point_unscaled: 44.4749 (47.2126)\n",
      "trn: mae=8.705128205128204, mse=12.773670837573146\n",
      "val: mae=11.153846153846153, mse=19.90554619272652\n",
      "EPOCH 151/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1447 (0.1683)  loss_ce: 0.1447 (0.1683)  loss_ce_unscaled: 0.1447 (0.1683)  loss_point_unscaled: 49.5217 (50.0736)\n",
      "EPOCH 152/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1696 (0.1876)  loss_ce: 0.1696 (0.1876)  loss_ce_unscaled: 0.1696 (0.1876)  loss_point_unscaled: 40.3815 (43.9800)\n",
      "EPOCH 153/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1526 (0.1692)  loss_ce: 0.1526 (0.1692)  loss_ce_unscaled: 0.1526 (0.1692)  loss_point_unscaled: 39.8719 (40.6758)\n",
      "EPOCH 154/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1350 (0.1494)  loss_ce: 0.1350 (0.1494)  loss_ce_unscaled: 0.1350 (0.1494)  loss_point_unscaled: 37.9490 (44.2477)\n",
      "EPOCH 155/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1315 (0.1383)  loss_ce: 0.1315 (0.1383)  loss_ce_unscaled: 0.1315 (0.1383)  loss_point_unscaled: 44.1979 (44.8621)\n",
      "EPOCH 156/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1592 (0.1598)  loss_ce: 0.1592 (0.1598)  loss_ce_unscaled: 0.1592 (0.1598)  loss_point_unscaled: 39.6173 (43.0923)\n",
      "EPOCH 157/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1558 (0.1600)  loss_ce: 0.1558 (0.1600)  loss_ce_unscaled: 0.1558 (0.1600)  loss_point_unscaled: 33.7383 (36.3144)\n",
      "EPOCH 158/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1696 (0.2119)  loss_ce: 0.1696 (0.2119)  loss_ce_unscaled: 0.1696 (0.2119)  loss_point_unscaled: 42.3747 (45.5202)\n",
      "EPOCH 159/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1518 (0.1693)  loss_ce: 0.1518 (0.1693)  loss_ce_unscaled: 0.1518 (0.1693)  loss_point_unscaled: 48.2819 (49.7806)\n",
      "EPOCH 160/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1452 (0.1588)  loss_ce: 0.1452 (0.1588)  loss_ce_unscaled: 0.1452 (0.1588)  loss_point_unscaled: 34.8955 (36.1282)\n",
      "trn: mae=8.435897435897436, mse=11.628876524874157\n",
      "val: mae=9.846153846153847, mse=17.549928774784245\n",
      "EPOCH 161/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1466 (0.1539)  loss_ce: 0.1466 (0.1539)  loss_ce_unscaled: 0.1466 (0.1539)  loss_point_unscaled: 45.1105 (49.5856)\n",
      "EPOCH 162/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1394 (0.1549)  loss_ce: 0.1394 (0.1549)  loss_ce_unscaled: 0.1394 (0.1549)  loss_point_unscaled: 40.9670 (41.8934)\n",
      "EPOCH 163/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1616 (0.1713)  loss_ce: 0.1616 (0.1713)  loss_ce_unscaled: 0.1616 (0.1713)  loss_point_unscaled: 44.7888 (53.8896)\n",
      "EPOCH 164/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1434 (0.1502)  loss_ce: 0.1434 (0.1502)  loss_ce_unscaled: 0.1434 (0.1502)  loss_point_unscaled: 40.5719 (41.8382)\n",
      "EPOCH 165/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1547 (0.1573)  loss_ce: 0.1547 (0.1573)  loss_ce_unscaled: 0.1547 (0.1573)  loss_point_unscaled: 44.2866 (47.0984)\n",
      "EPOCH 166/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1621 (0.1637)  loss_ce: 0.1621 (0.1637)  loss_ce_unscaled: 0.1621 (0.1637)  loss_point_unscaled: 36.1651 (43.1559)\n",
      "EPOCH 167/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1374 (0.1642)  loss_ce: 0.1374 (0.1642)  loss_ce_unscaled: 0.1374 (0.1642)  loss_point_unscaled: 39.1468 (46.9025)\n",
      "EPOCH 168/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1606 (0.1726)  loss_ce: 0.1606 (0.1726)  loss_ce_unscaled: 0.1606 (0.1726)  loss_point_unscaled: 37.7583 (42.2627)\n",
      "EPOCH 169/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1225 (0.1472)  loss_ce: 0.1225 (0.1472)  loss_ce_unscaled: 0.1225 (0.1472)  loss_point_unscaled: 37.4092 (40.4375)\n",
      "EPOCH 170/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1545 (0.1583)  loss_ce: 0.1545 (0.1583)  loss_ce_unscaled: 0.1545 (0.1583)  loss_point_unscaled: 43.2859 (46.4143)\n",
      "trn: mae=6.961538461538462, mse=10.226285888675472\n",
      "val: mae=11.076923076923077, mse=18.409863576981685\n",
      "EPOCH 171/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1681 (0.1727)  loss_ce: 0.1681 (0.1727)  loss_ce_unscaled: 0.1681 (0.1727)  loss_point_unscaled: 44.3681 (49.8726)\n",
      "EPOCH 172/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1545 (0.1579)  loss_ce: 0.1545 (0.1579)  loss_ce_unscaled: 0.1545 (0.1579)  loss_point_unscaled: 42.0114 (43.6318)\n",
      "EPOCH 173/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1576 (0.1729)  loss_ce: 0.1576 (0.1729)  loss_ce_unscaled: 0.1576 (0.1729)  loss_point_unscaled: 44.6192 (49.4172)\n",
      "EPOCH 174/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1802 (0.1867)  loss_ce: 0.1802 (0.1867)  loss_ce_unscaled: 0.1802 (0.1867)  loss_point_unscaled: 43.3011 (49.7117)\n",
      "EPOCH 175/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1535 (0.1564)  loss_ce: 0.1535 (0.1564)  loss_ce_unscaled: 0.1535 (0.1564)  loss_point_unscaled: 44.3169 (47.4567)\n",
      "EPOCH 176/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1688 (0.1848)  loss_ce: 0.1688 (0.1848)  loss_ce_unscaled: 0.1688 (0.1848)  loss_point_unscaled: 44.9959 (46.9519)\n",
      "EPOCH 177/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1239 (0.1387)  loss_ce: 0.1239 (0.1387)  loss_ce_unscaled: 0.1239 (0.1387)  loss_point_unscaled: 41.1292 (41.5669)\n",
      "EPOCH 178/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1491 (0.1863)  loss_ce: 0.1491 (0.1863)  loss_ce_unscaled: 0.1491 (0.1863)  loss_point_unscaled: 41.2082 (41.7154)\n",
      "EPOCH 179/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1316 (0.1370)  loss_ce: 0.1316 (0.1370)  loss_ce_unscaled: 0.1316 (0.1370)  loss_point_unscaled: 43.9142 (44.0345)\n",
      "EPOCH 180/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1425 (0.1546)  loss_ce: 0.1425 (0.1546)  loss_ce_unscaled: 0.1425 (0.1546)  loss_point_unscaled: 42.7505 (44.0827)\n",
      "trn: mae=10.0, mse=13.924705953715357\n",
      "val: mae=10.461538461538462, mse=18.613477333951828\n",
      "EPOCH 181/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1239 (0.1414)  loss_ce: 0.1239 (0.1414)  loss_ce_unscaled: 0.1239 (0.1414)  loss_point_unscaled: 39.8657 (44.2948)\n",
      "EPOCH 182/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1500 (0.1665)  loss_ce: 0.1500 (0.1665)  loss_ce_unscaled: 0.1500 (0.1665)  loss_point_unscaled: 46.3161 (49.8779)\n",
      "EPOCH 183/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1592 (0.1615)  loss_ce: 0.1592 (0.1615)  loss_ce_unscaled: 0.1592 (0.1615)  loss_point_unscaled: 41.2511 (42.9981)\n",
      "EPOCH 184/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1422 (0.1543)  loss_ce: 0.1422 (0.1543)  loss_ce_unscaled: 0.1422 (0.1543)  loss_point_unscaled: 46.1788 (46.6462)\n",
      "EPOCH 185/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1382 (0.1426)  loss_ce: 0.1382 (0.1426)  loss_ce_unscaled: 0.1382 (0.1426)  loss_point_unscaled: 39.1582 (39.2376)\n",
      "EPOCH 186/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1211 (0.1303)  loss_ce: 0.1211 (0.1303)  loss_ce_unscaled: 0.1211 (0.1303)  loss_point_unscaled: 39.8168 (42.1285)\n",
      "EPOCH 187/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1518 (0.1538)  loss_ce: 0.1518 (0.1538)  loss_ce_unscaled: 0.1518 (0.1538)  loss_point_unscaled: 41.9247 (47.0681)\n",
      "EPOCH 188/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1295 (0.1322)  loss_ce: 0.1295 (0.1322)  loss_ce_unscaled: 0.1295 (0.1322)  loss_point_unscaled: 38.5317 (40.8553)\n",
      "EPOCH 189/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1412 (0.1615)  loss_ce: 0.1412 (0.1615)  loss_ce_unscaled: 0.1412 (0.1615)  loss_point_unscaled: 36.4641 (40.3616)\n",
      "EPOCH 190/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1398 (0.1430)  loss_ce: 0.1398 (0.1430)  loss_ce_unscaled: 0.1398 (0.1430)  loss_point_unscaled: 43.7609 (46.1727)\n",
      "trn: mae=8.935897435897436, mse=12.095241700556123\n",
      "val: mae=10.23076923076923, mse=18.023488947652556\n",
      "EPOCH 191/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1470 (0.1519)  loss_ce: 0.1470 (0.1519)  loss_ce_unscaled: 0.1470 (0.1519)  loss_point_unscaled: 44.1329 (48.6085)\n",
      "EPOCH 192/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1247 (0.1393)  loss_ce: 0.1247 (0.1393)  loss_ce_unscaled: 0.1247 (0.1393)  loss_point_unscaled: 43.2909 (43.3517)\n",
      "EPOCH 193/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1271 (0.1410)  loss_ce: 0.1271 (0.1410)  loss_ce_unscaled: 0.1271 (0.1410)  loss_point_unscaled: 36.7505 (41.5197)\n",
      "EPOCH 194/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1406 (0.1550)  loss_ce: 0.1406 (0.1550)  loss_ce_unscaled: 0.1406 (0.1550)  loss_point_unscaled: 40.2464 (47.1846)\n",
      "EPOCH 195/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1413 (0.1533)  loss_ce: 0.1413 (0.1533)  loss_ce_unscaled: 0.1413 (0.1533)  loss_point_unscaled: 46.2841 (49.3760)\n",
      "EPOCH 196/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1487 (0.1729)  loss_ce: 0.1487 (0.1729)  loss_ce_unscaled: 0.1487 (0.1729)  loss_point_unscaled: 44.3832 (45.1767)\n",
      "EPOCH 197/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1343 (0.1455)  loss_ce: 0.1343 (0.1455)  loss_ce_unscaled: 0.1343 (0.1455)  loss_point_unscaled: 43.9914 (45.3434)\n",
      "EPOCH 198/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1384 (0.1742)  loss_ce: 0.1384 (0.1742)  loss_ce_unscaled: 0.1384 (0.1742)  loss_point_unscaled: 41.7840 (41.7897)\n",
      "EPOCH 199/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1491 (0.1691)  loss_ce: 0.1491 (0.1691)  loss_ce_unscaled: 0.1491 (0.1691)  loss_point_unscaled: 37.7826 (41.4440)\n",
      "EPOCH 200/200\n",
      "Averaged stats: lr: 0.000100  loss: 0.1287 (0.1321)  loss_ce: 0.1287 (0.1321)  loss_ce_unscaled: 0.1287 (0.1321)  loss_point_unscaled: 35.3055 (40.1480)\n",
      "trn: mae=10.320512820512821, mse=13.546217184144066\n",
      "val: mae=12.23076923076923, mse=18.349177302874036\n",
      "val idxs: [11, 12, 14, 23, 26, 29, 33, 45, 60, 61, 62, 74, 79]\n"
     ]
    }
   ],
   "source": [
    "seed_all()\n",
    "folds = torch.randperm(len(base_dset)).reshape(K, -1)\n",
    "# wipe vis directory for val outputs\n",
    "for f in os.listdir(VIS_DIR):\n",
    "    if f.endswith('jpg'):\n",
    "        os.remove(f\"{VIS_DIR}/{f}\")\n",
    "for k in range(1):\n",
    "    print(f\">>> FOLD {k+1}/{K} <<<\")\n",
    "    train_dl, trn_evaldl, val_dl = get_subset_dataloaders(base_dset, folds, k, bsz=hparams.bsz, dl_kwargs=dl_kwargs)\n",
    "    model, criterion = load_new_model(model_args, device, pretrain_layers=hparams.pretrain_layers)\n",
    "    opt_params = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"backbone\" not in n and p.requires_grad],\n",
    "            \"lr\": hparams.lr_gen\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "            \"lr\": hparams.lr_backbone,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.Adam(opt_params)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, hparams.lr_drop) # basically does nothing\n",
    "    for attr in ['train', 'patch', 'flip']:\n",
    "        base_dset.__setattr__(attr, True)\n",
    "    for epoch in range(hparams.epochs):\n",
    "        print(f\"EPOCH {epoch+1}/{hparams.epochs}\")\n",
    "        stats = train_one_epoch(model, criterion, train_dl, optimizer, device, \\\n",
    "                                epoch, hparams.clip_max_norm)\n",
    "        if epoch and (epoch+1) % hparams.val_every == 0:\n",
    "            # HACK: disable and then re-enable training-time args for the dataset\n",
    "            for attr in ['train', 'patch', 'flip']:\n",
    "                base_dset.__setattr__(attr, False)\n",
    "            # visualize final results only\n",
    "            vis_dir = VIS_DIR if hparams.val_every + epoch >= hparams.epochs else None\n",
    "            t_mae, t_mse = evaluate_crowd_no_overlap(model, trn_evaldl, device, vis_dir=vis_dir)\n",
    "            print(f\"trn: mae={t_mae}, mse={t_mse}\")\n",
    "            v_mae, v_mse = evaluate_crowd_no_overlap(model, val_dl, device, vis_dir=vis_dir)\n",
    "            print(f\"val: mae={v_mae}, mse={v_mse}\")\n",
    "            # reset stuff for training\n",
    "            for attr in ['train', 'patch', 'flip']:\n",
    "                base_dset.__setattr__(attr, True)\n",
    "    print(\"val idxs:\", sorted(folds[k].tolist()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fixing crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'cropped/'\n",
    "images = [f for f in os.listdir(path)\n",
    "          if os.path.isfile(path+f) and f.endswith('.jpg')]\n",
    "THRESH = 5 # need more than THRESH nonzero values to be considered valid\n",
    "BLACK_THRESH = 100\n",
    "MIN_IMG_DIM = 128\n",
    "\n",
    "# finds the first index of 1D array x that surpasses the threshold\n",
    "find_first = lambda x: (x > THRESH).argmax()\n",
    "for img_name in images:\n",
    "    img = cv2.imread(path+img_name)\n",
    "    nonblack_mask = (img > BLACK_THRESH)\n",
    "    img_1ch = nonblack_mask.any(axis=2).astype(int) # flatten channels\n",
    "    first_good_x = find_first(img_1ch.sum(axis=0))\n",
    "    first_good_y = find_first(img_1ch.sum(axis=1))\n",
    "    # crop\n",
    "    cropped_img = img\n",
    "    if first_good_x:\n",
    "        cropped_img = cropped_img[:, first_good_x:-first_good_x]\n",
    "    if first_good_y:\n",
    "        cropped_img = cropped_img[first_good_y:-first_good_y]\n",
    "    roi = cropped_img\n",
    "    min_dim = min(roi.shape[0], roi.shape[1])\n",
    "    scale = max(1, MIN_IMG_DIM/min_dim)\n",
    "    scale_up = lambda l: int(np.ceil(scale*l))\n",
    "    roi_out = cv2.resize(roi, (scale_up(roi.shape[1]), scale_up(roi.shape[0])))\n",
    "    # fix coordinates\n",
    "    # label_path = path+img_name[:-len(\"anno.jpg\")]+\"lbl.txt\"\n",
    "    # with open(label_path, 'r') as f:\n",
    "    #     coords_list = f.readlines()\n",
    "    #     coords = [tuple(map(int, line.split())) for line in coords_list]\n",
    "    #     fixed_coords = [(scale_up(x-first_good_x), scale_up(y-first_good_y)) for (x,y) in coords]\n",
    "    # label_str = '\\n'.join([f\"{x} {y}\" for (x,y) in fixed_coords])\n",
    "    # overwrite files\n",
    "    cv2.imwrite(path+img_name, roi_out)\n",
    "    # with open(label_path, 'w') as f:\n",
    "    #     f.write(label_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ag_ct",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
